{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install GOFevaluation\n",
    "#pip install goodness-of-fit\n",
    "#pip install torchinfo\n",
    "#pip install netron\n",
    "#pip install onnx\n",
    "#pip install onnxscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "import onnx\n",
    "import netron\n",
    "\n",
    "import GOFevaluation as ge\n",
    "from goodness_of_fit import gof_measure \n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined download folder for pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Construct the path to the 'model' folder within your project directory\n",
    "model_dir = os.path.join(os.getcwd(), 'models')\n",
    "\n",
    "# Set the TORCH_HOME environment variable to the 'model' directory path\n",
    "os.environ['TORCH_HOME'] = model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pretrained model and observe its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pre-trained ResNet-50 Model\n",
    "model = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet-50 structure:\n",
    "\n",
    "![Different ResNet Architecture](https://miro.medium.com/v2/resize:fit:700/1*IP9PgUQmkhJiFRdIW6TwMQ.png)\n",
    "\n",
    "The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as ResNet V1.5. See:\n",
    "[Pytorch pre-trained resnet-50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html)\n",
    "\n",
    "![Resnet-50-1](https://www.researchgate.net/profile/Anastasios-Stamoulakatos/publication/338833936/figure/fig3/AS:851920512110592@1580125086957/ResNet-50-architecture-with-modified-head.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all modules, including nested ones\n",
    "for name, _ in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the loaded pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (1, 3, 224, 224), depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_input = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()  # Move the model to CUDA\n",
    "torch_input = torch_input.cuda()  # Move the input tensor to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx_program = torch.onnx.dynamo_export(model, torch_input)\n",
    "onnx_path=\"./models/resnet50_pretrained.onnx\"\n",
    "# onnx_program.save(onnx_path)\n",
    "torch.onnx.export(model, torch_input, onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx_model = onnx.load(onnx_path)\n",
    "# onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# visualize the model structure using netron\n",
    "netron.start(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop visualization server\n",
    "netron.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load and Transform the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(dataset,dataset_path,mean,std,num_samples=100):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224),  # Resize images to 224x224\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    if dataset==\"CIFAR-10\":\n",
    "        # Load CIFAR-10\n",
    "        trainset = torchvision.datasets.CIFAR10(root=dataset_path, train=True,\n",
    "                                                download=True, transform=transform)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=dataset_path, train=False,\n",
    "                                            download=True, transform=transform)\n",
    "    elif dataset==\"CIFAR-100\":\n",
    "        # Load CIFAR-10\n",
    "        trainset = torchvision.datasets.CIFAR100(root=dataset_path, train=True,\n",
    "                                                download=True, transform=transform)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR100(root=dataset_path, train=False,\n",
    "                                            download=True, transform=transform)\n",
    "    else: \n",
    "         return \"Invaid Dataset.\"\n",
    "    # Concatenate the training and testing datasets\n",
    "    combined_set = torch.utils.data.ConcatDataset([trainset, testset])\n",
    "    \n",
    "     # Generate random indices to sample from the dataset\n",
    "    indices = np.random.choice(len(combined_set), num_samples, replace=False)\n",
    "    \n",
    "    # Create a subset based on the generated indices\n",
    "    sampled_subset = torch.utils.data.Subset(combined_set, indices)\n",
    "    \n",
    "    # Create a single DataLoader from the combined dataset\n",
    "    loader = torch.utils.data.DataLoader(sampled_subset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the image pixel values based on the mean and standard deviation of the ImageNet dataset across the RGB channels. \n",
    "The purpose of this normalization is to transform the input images so that their pixel value distribution more closely resembles that of the original ImageNet training data. By doing this, we put CIFAR-10 images on a similar scale and distribution as the ImageNet images, which can help the pre-trained model make better predictions or extract more relevant features from the CIFAR-10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and standard deviation of the ImageNet dataset across the RGB channels\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "dataset=\"CIFAR-10\"\n",
    "num_samples=10\n",
    "# Define custom path for the CIFAR dataset\n",
    "dataset_path = '../datasets/CIFAR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=data_loading(dataset,dataset_path,mean,std,num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_fclayer(dataset,model):\n",
    "    if dataset==\"CIFAR-10\":\n",
    "        # Modify the ResNet-50 Model for CIFAR\n",
    "        num_ftrs = model.fc.in_features  # Get the number of features in the last layer\n",
    "        model.fc = nn.Linear(num_ftrs, 10)  # Replace the last layer\n",
    "        \n",
    "    elif dataset==\"CIFAR-100\":\n",
    "        # Modify the ResNet-50 Model for CIFAR\n",
    "        num_ftrs = model.fc.in_features  # Get the number of features in the last layer\n",
    "        model.fc = nn.Linear(num_ftrs, 100)  # Replace the last layer\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=modify_fclayer(dataset,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define selected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_layer_names =[\n",
    "#     'layer1.0.conv2','layer1.1.conv2','layer1.2.conv2',\n",
    "#     'layer2.0.conv2','layer2.1.conv2','layer2.2.conv2','layer2.3.conv2',\n",
    "#     'layer3.0.conv2','layer3.1.conv2','layer3.2.conv2','layer3.3.conv2','layer3.4.conv2','layer3.5.conv2',\n",
    "#     'layer4.0.conv2','layer4.1.conv2','layer4.2.conv2'\n",
    "# ]\n",
    "\n",
    "selected_layer_names =[\n",
    "    'layer1.0.relu','layer1.1.relu','layer1.2.relu',\n",
    "    'layer2.0.relu','layer2.1.relu','layer2.2.relu','layer2.3.relu',\n",
    "    'layer3.0.relu','layer3.1.relu','layer3.2.relu','layer3.3.relu','layer3.4.relu','layer3.5.relu',\n",
    "    'layer4.0.relu','layer4.1.relu','layer4.2.relu'\n",
    "]\n",
    "\n",
    "layer_names_for_saving =[\n",
    "    'Bottleneck2-1','Bottleneck2-2','Bottleneck2-3',\n",
    "    'Bottleneck3-1','Bottleneck3-2','Bottleneck3-3','Bottleneck3-4',\n",
    "    'Bottleneck4-1','Bottleneck4-2','Bottleneck4-3','Bottleneck4-4','Bottleneck4-5','Bottleneck4-6',\n",
    "    'Bottleneck5-1','Bottleneck5-2','Bottleneck5-3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_layers(model,selected_layer_names):\n",
    "    selected_layers = []\n",
    "    # Iterate over all modules\n",
    "    for name, module in model.named_modules():\n",
    "        if name in selected_layer_names:\n",
    "            selected_layers.append(module)\n",
    "            \n",
    "    return selected_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_layers=select_layers(model,selected_layer_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define goodness-of-fit metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_gof_list=[\n",
    "    'ADTestTwoSampleGOF',\n",
    "    'KSTestTwoSampleGOF',\n",
    "    'PointToPointGOF',\n",
    "]\n",
    "\n",
    "binned_gof_list=[\n",
    "    # 'KSTestGOF',  # need bin edges\n",
    "    # 'BinnedPoissonChi2GOF', # need pdf\n",
    "    # 'BinnedChi2GOF'   # need pdf\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define PDF types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_type={\n",
    "    'gaussian':sps.norm.rvs,\n",
    "    'laplace':sps.laplace.rvs,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF resample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_and_bin(data, pdf_type, bins=10):\n",
    "    # Flatten the N-D array to 1-D\n",
    "    data_flattened = data.ravel()\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean = np.mean(data_flattened)\n",
    "    std = np.std(data_flattened)\n",
    "    \n",
    "    outputs = {}\n",
    "    for dist_name, dist_func in pdf_type.items():\n",
    "        # Fit and resample from the specified distribution\n",
    "        if dist_name == 'gaussian':\n",
    "            samples = dist_func(loc=mean, scale=std, size=len(data_flattened),random_state = 42 )\n",
    "        elif dist_name == 'laplace':\n",
    "            # Scale parameter for Laplace is std / sqrt(2) according to the definition\n",
    "            samples = dist_func(loc=mean, scale=std / np.sqrt(2), size=len(data_flattened),random_state = 42 )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported distribution: {dist_name}\")\n",
    "\n",
    "         # Bin the data from the current distribution and capture the bin edges\n",
    "        binned, bin_edges = np.histogram(samples, bins=bins, density=True)\n",
    "        \n",
    "        # Store the sampled array, its binned version, and the bin edges in the outputs dictionary\n",
    "        outputs[dist_name] = {'samples': samples, 'binned': binned, 'bin_edges': bin_edges}\n",
    "    \n",
    "    # Output the dictionary containing sampled arrays and their binned versions for all distributions\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate goodness-of-fit scores on one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gof_calculation(sample_gof_list, binned_gof_list, data, reference):\n",
    "    gof_scores = {}\n",
    "    \n",
    "    # Iterate through each pdf_name in the reference\n",
    "    for pdf_name in reference.keys():\n",
    "        # Calculate sample-based GOF for 'samples'\n",
    "        if 'samples' in reference[pdf_name]:\n",
    "            samples = reference[pdf_name]['samples']\n",
    "            gof_object = ge.GOFTest(data_sample=data, reference_sample=samples, gof_list=sample_gof_list)  # Using data as both sample and reference\n",
    "            gofs=gof_object.get_gofs(d_min=0.01)  # Adjust d_min based on your requirement\n",
    "            for gof_name in sample_gof_list:\n",
    "                # Concatenate pdf_name with gof_name for the key\n",
    "                new_key_name = f\"{pdf_name}_{gof_name}\"\n",
    "                gof_scores[new_key_name] = gofs[gof_name]\n",
    "        \n",
    "        # Calculate binned-based GOF for 'binned'\n",
    "        if 'binned' in reference[pdf_name]:\n",
    "            samples_binned = reference[pdf_name]['binned']\n",
    "            bin_edges = reference[pdf_name]['bin_edges']\n",
    "            # Handle \"KSTestGOF\" separately\n",
    "            if 'KSTestGOF' in binned_gof_list:\n",
    "                gof_object_ks = ge.KSTestGOF(data, samples_binned,bin_edges)\n",
    "                gof_ks = gof_object_ks.get_gof()\n",
    "                new_key_name_ks = f\"{pdf_name}_KSTestGOF_binned\"\n",
    "                gof_scores[new_key_name_ks] = gof_ks\n",
    "\n",
    "           # Prepare a list excluding \"KSTestGOF\" for the remaining calculations\n",
    "            other_binned_gof_list = [gof_name for gof_name in binned_gof_list if gof_name != 'KSTestGOF']\n",
    "\n",
    "            # Proceed with the remaining binned GOF measures\n",
    "            if other_binned_gof_list:\n",
    "                gof_object_binned = ge.GOFTest(data_sample=data, reference_sample=samples_binned, gof_list=other_binned_gof_list)\n",
    "                gofs = gof_object_binned.get_gofs(d_min=0.01)  # Adjust d_min based on your requirement\n",
    "                for gof_name in other_binned_gof_list:\n",
    "                    # Concatenate pdf_name with gof_name for the key\n",
    "                    new_key_name = f\"{pdf_name}_{gof_name}_binned\"\n",
    "                    gof_scores[new_key_name] = gofs[gof_name]\n",
    "\n",
    "    # GOF measures based on the goodness_of_fit package\n",
    "    for gof_name, gof_func in gof_measure.items():\n",
    "        for pdf_name in reference.keys():\n",
    "            if 'samples' in reference[pdf_name]:\n",
    "                samples = reference[pdf_name]['samples']\n",
    "                # Concatenate pdf_name with gof_name for the key\n",
    "                new_key_name = f\"{pdf_name}_{gof_name}\"\n",
    "                gof_scores[new_key_name] = gof_func(samples, data)  # Using data as both observed and calculated values\n",
    "    \n",
    "    return gof_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate gof scores of one feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map_gof_calculation(feature_map, pdf_tpye,sample_gof_list, binned_gof_list):\n",
    "    \"\"\"\n",
    "    Calculate GOF scores for each channel in a feature map and average the scores.\n",
    "\n",
    "    :param feature_map: A numpy array of shape (W, H, C) representing the feature map.\n",
    "    :param sample_gof_list: A list of sample-based GOF measures.\n",
    "    :param binned_gof_list: A list of binned-based GOF measures.\n",
    "    :param gof_measure: A dictionary of other GOF measures.\n",
    "    :return: A dictionary of averaged GOF scores.\n",
    "    \"\"\"\n",
    "    W, H, C = feature_map.shape\n",
    "    gof_scores_sum = {}\n",
    "\n",
    "    # Iterate through each channel\n",
    "    for c in range(C):\n",
    "        # Reshape W*H samples into a 1D array for the current channel\n",
    "        epsilon = 1e-8  # small value\n",
    "        data_sample = feature_map[:, :, c].reshape(-1) + epsilon\n",
    "        # data_sample = feature_map[:, :, c].reshape(-1)\n",
    "        # data_sample = data_sample[data_sample != 0]\n",
    "        \n",
    "        # Assuming 'reference' contains predefined distributions for GOF comparison\n",
    "        # Here 'reference' needs to be defined or passed to the function based on your setup\n",
    "        reference = resample_and_bin(data_sample, pdf_tpye, 30)\n",
    "\n",
    "        # Calculate GOF scores for the current channel\n",
    "        gof_scores_channel = gof_calculation(sample_gof_list, binned_gof_list, data_sample, reference)\n",
    "\n",
    "        # Sum the GOF scores for averaging later\n",
    "        for gof_name, score in gof_scores_channel.items():\n",
    "            if gof_name not in gof_scores_sum:\n",
    "                gof_scores_sum[gof_name] = 0\n",
    "            gof_scores_sum[gof_name] += score\n",
    "\n",
    "    # Calculate the average GOF score across all channels\n",
    "    gof_scores_avg = {gof_name: score / C for gof_name, score in gof_scores_sum.items()}\n",
    "\n",
    "    return gof_scores_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract feature maps according to the selected layer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_maps(model, selected_layer_names, input_image):\n",
    "    feature_maps = {}\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # Find the layer's name in the model by matching the module object\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                feature_maps[name] = output.detach()\n",
    "                break\n",
    "\n",
    "    hooks = []\n",
    "    # Register hooks using the layer names\n",
    "    for name, module in model.named_modules():\n",
    "        # print(name) \n",
    "        if name in selected_layer_names:\n",
    "            # print('find')\n",
    "            hook = module.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    # Forward pass to extract feature maps\n",
    "    model(input_image)\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return feature_maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the gof scores on all feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gof_scores(feature_maps, pdf_type, sample_gof_list, binned_gof_list, selected_layer_names, layer_names_for_saving):\n",
    "    # Initialize an empty DataFrame for all scores\n",
    "    all_scores_df = pd.DataFrame()\n",
    "    \n",
    "    for layer_name, fmap in feature_maps.items():\n",
    "        # Squeeze batch dimension if necessary (assuming the batch size is always 1)\n",
    "        if fmap.dim() > 3:\n",
    "            fmap = fmap.squeeze(0)\n",
    "        \n",
    "        # Now fmap should be in the format of C x H x W, so we can get the number of channels directly\n",
    "        num_channels = fmap.size(0)  # The number of channels is the size of the first dimension\n",
    "        \n",
    "        print(f\"Layer: {layer_name}, Number of Channels: {num_channels}\")\n",
    "\n",
    "    for layer_name, fmap in feature_maps.items():\n",
    "        # Squeeze batch dimension, move data to CPU, and convert to numpy\n",
    "        fmap = fmap.squeeze(0).cpu().numpy()\n",
    "        # Calculate GOF scores for the feature map\n",
    "        fmap_scores = feature_map_gof_calculation(fmap, pdf_type, sample_gof_list, binned_gof_list)\n",
    "        # Convert the fmap_scores dictionary into a DataFrame\n",
    "        # 'gof_scores' as index and one column named 'score' with the GOF scores\n",
    "        fmap_df = pd.DataFrame(list(fmap_scores.items()), columns=['gof_scores', 'score'])\n",
    "        # print(fmap_df)\n",
    "\n",
    "        # Map the layer name to the corresponding descriptive name for saving\n",
    "        layer_index = selected_layer_names.index(layer_name)\n",
    "        descriptive_layer_name = layer_names_for_saving[layer_index]\n",
    "        \n",
    "        # Rename the 'score' column to the descriptive layer name\n",
    "        fmap_df.rename(columns={'score': descriptive_layer_name}, inplace=True)\n",
    "\n",
    "        # If this is the first layer, initialize all_scores_df with fmap_df\n",
    "        if all_scores_df.empty:\n",
    "            all_scores_df = fmap_df\n",
    "        else:\n",
    "            # Merge the current fmap_df with all_scores_df on the 'gof_scores' column\n",
    "            # This aligns the scores for the same GOF metrics across different layers\n",
    "            all_scores_df = pd.merge(all_scores_df, fmap_df, on='gof_scores', how='outer')\n",
    "\n",
    "    return all_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate gof scores on all sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gof_scores_for_dataloader(model, loader, selected_layer_names, pdf_type, sample_gof_list, binned_gof_list, layer_names_for_saving):\n",
    "    \"\"\"\n",
    "    Calculate GOF scores for each image in the DataLoader, for selected layers of the model.\n",
    "\n",
    "    :param model: The neural network model.\n",
    "    :param loader: The DataLoader containing the dataset.\n",
    "    :param selected_layers: The selected layers of the model for which to calculate GOF scores.\n",
    "    :param pdf_type: The PDF types used for resampling.\n",
    "    :param sample_gof_list: A list of sample-based GOF measures.\n",
    "    :param binned_gof_list: A list of binned-based GOF measures.\n",
    "    :param layer_names_for_saving: The layer names corresponding to the selected layers, used for saving in DataFrame.\n",
    "    :return: A DataFrame with averaged GOF scores across all images in the DataLoader.\n",
    "    \"\"\"\n",
    "    total_df = pd.DataFrame()\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over the DataLoader\n",
    "    for i, (input_images, _) in enumerate(loader):\n",
    "        input_images = input_images.to(device)\n",
    "        print(f\"Processing batch {i+1}/{len(loader)}...\")\n",
    "\n",
    "        for j, input_image in enumerate(input_images):\n",
    "            input_image = input_image.unsqueeze(0)  # Add batch dimension if necessary\n",
    "            input_image = input_image.to(device)  # Move to the same device as the model\n",
    "\n",
    "            # Extract feature maps for the current image\n",
    "            feature_maps = extract_feature_maps(model, selected_layer_names, input_image)\n",
    "            \n",
    "            # print(len(feature_maps))\n",
    "\n",
    "            # Compute GOF scores for the extracted feature maps\n",
    "            scores_df = compute_gof_scores(feature_maps, pdf_type, sample_gof_list, binned_gof_list, selected_layer_names, layer_names_for_saving)\n",
    "\n",
    "            # Sum the scores across all processed images for averaging later\n",
    "            if total_df.empty:\n",
    "                total_df = scores_df\n",
    "            else:\n",
    "                # Sum corresponding scores\n",
    "                for col in layer_names_for_saving:\n",
    "                    total_df[col] += scores_df[col]\n",
    "    \n",
    "\n",
    "    # Compute the average scores by dividing the summed scores by the number of processed images\n",
    "    total_df[layer_names_for_saving] = total_df[layer_names_for_saving].div(len(loader.dataset))\n",
    "\n",
    "    # Compute the overall score by averaging across layers for each GOF metric after summing\n",
    "    for gof_name in total_df['gof_scores'].unique():\n",
    "        total_df.loc[total_df['gof_scores'] == gof_name, 'overall'] = total_df[total_df['gof_scores'] == gof_name][layer_names_for_saving].mean(axis=1)\n",
    "\n",
    "    return total_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_average_df = calculate_gof_scores_for_dataloader(model, loader, selected_layer_names, pdf_type, sample_gof_list, binned_gof_list, layer_names_for_saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulate dataframe for friendly reading before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract metric name and distribution from the row name\n",
    "def extract_metric_and_dist(row_name):\n",
    "    parts = row_name.split('_')\n",
    "    dist = parts[0]  # e.g., \"gaussian\" or \"laplace\"\n",
    "    metric = '_'.join(parts[1:])  # e.g., \"ADTestTwoSampleGOF\"\n",
    "    return metric, dist\n",
    "\n",
    "# Custom sort key that prioritizes the metric name and then the distribution name\n",
    "def custom_sort_key(row_name):\n",
    "    metric, dist = extract_metric_and_dist(row_name)\n",
    "    return (metric, dist)\n",
    "\n",
    "# Ensure 'gof_scores' is set as the index if it's not already\n",
    "if total_average_df.index.name != 'gof_scores':\n",
    "    total_average_df = total_average_df.set_index('gof_scores')\n",
    "\n",
    "# Sort the DataFrame using the custom sort key\n",
    "sorted_index = sorted(total_average_df.index, key=custom_sort_key)\n",
    "sorted_df = total_average_df.loc[sorted_index]\n",
    "\n",
    "# Optionally, if you want to reset the index to make 'gof_scores' a column again\n",
    "sorted_df = sorted_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path=f'./result/{dataset}/total_average_scores.csv'\n",
    "os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "sorted_df.to_csv(result_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(sorted_df, dataset_name):\n",
    "    # Number of subplots to create\n",
    "    num_plots = len(sorted_df) // 2\n",
    "    \n",
    "    # Setting up the figure and subplots\n",
    "    fig, axs = plt.subplots(num_plots, 1, figsize=(10, 5 * num_plots))\n",
    "    \n",
    "    # If there's only one plot, axs may not be an array, so we wrap it in a list for consistency\n",
    "    if num_plots == 1:\n",
    "        axs = [axs]\n",
    "    \n",
    "    # Iterate through every 2 rows\n",
    "    for i in range(0, len(sorted_df), 2):\n",
    "        row1, row2 = sorted_df.iloc[i], sorted_df.iloc[i+1]\n",
    "        metric_name = row1['gof_scores'].split('_')[1]  # Assuming metric name is after the first underscore\n",
    "        \n",
    "        # Plotting the two rows as lines on the subplot\n",
    "        axs[i//2].plot(sorted_df.columns[1:], row1[1:], label=row1['gof_scores'])  # Skip the 'gof_scores' column\n",
    "        axs[i//2].plot(sorted_df.columns[1:], row2[1:], label=row2['gof_scores'])  # Skip the 'gof_scores' column\n",
    "        \n",
    "        # Setting x-axis labels to column names\n",
    "        axs[i//2].set_xticks(range(len(sorted_df.columns[1:])))\n",
    "        axs[i//2].set_xticklabels(sorted_df.columns[1:], rotation=45, ha=\"right\")\n",
    "        \n",
    "        # Setting the subplot title\n",
    "        axs[i//2].set_title(f\"{dataset_name} {metric_name}\")\n",
    "        \n",
    "        # Adding a legend to distinguish the two lines\n",
    "        axs[i//2].legend()\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    fig.savefig(f'./result/{dataset_name}/metrics_comparison.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(sorted_df, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"CIFAR-100\"\n",
    "loader=data_loading(dataset,dataset_path,mean,std,num_samples)\n",
    "model=modify_fclayer(dataset,model)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_average_df = calculate_gof_scores_for_dataloader(model, loader, selected_layer_names, pdf_type, sample_gof_list, binned_gof_list, layer_names_for_saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract metric name and distribution from the row name\n",
    "def extract_metric_and_dist(row_name):\n",
    "    parts = row_name.split('_')\n",
    "    dist = parts[0]  # e.g., \"gaussian\" or \"laplace\"\n",
    "    metric = '_'.join(parts[1:])  # e.g., \"ADTestTwoSampleGOF\"\n",
    "    return metric, dist\n",
    "\n",
    "# Custom sort key that prioritizes the metric name and then the distribution name\n",
    "def custom_sort_key(row_name):\n",
    "    metric, dist = extract_metric_and_dist(row_name)\n",
    "    return (metric, dist)\n",
    "\n",
    "# Ensure 'gof_scores' is set as the index if it's not already\n",
    "if total_average_df.index.name != 'gof_scores':\n",
    "    total_average_df = total_average_df.set_index('gof_scores')\n",
    "\n",
    "# Sort the DataFrame using the custom sort key\n",
    "sorted_index = sorted(total_average_df.index, key=custom_sort_key)\n",
    "sorted_df = total_average_df.loc[sorted_index]\n",
    "\n",
    "# Optionally, if you want to reset the index to make 'gof_scores' a column again\n",
    "sorted_df = sorted_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path=f'./result/{dataset}/total_average_scores.csv'\n",
    "os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "sorted_df.to_csv(result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(sorted_df, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Construct the path to the 'model' folder within your project directory\n",
    "model_dir = os.path.join(os.getcwd(), 'models')\n",
    "\n",
    "# Set the TORCH_HOME environment variable to the 'model' directory path\n",
    "os.environ['TORCH_HOME'] = model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifeng/miniconda3/envs/attention/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yifeng/miniconda3/envs/attention/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the Pre-trained ResNet-50 Model\n",
    "model = torchvision.models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the fully connected (FC) layer by modifying the `fc` attribute\n",
    "model.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer1.0\n",
      "layer1.0.conv1\n",
      "layer1.0.bn1\n",
      "layer1.0.relu\n",
      "layer1.0.conv2\n",
      "layer1.0.bn2\n",
      "layer1.1\n",
      "layer1.1.conv1\n",
      "layer1.1.bn1\n",
      "layer1.1.relu\n",
      "layer1.1.conv2\n",
      "layer1.1.bn2\n",
      "layer1.2\n",
      "layer1.2.conv1\n",
      "layer1.2.bn1\n",
      "layer1.2.relu\n",
      "layer1.2.conv2\n",
      "layer1.2.bn2\n",
      "layer2\n",
      "layer2.0\n",
      "layer2.0.conv1\n",
      "layer2.0.bn1\n",
      "layer2.0.relu\n",
      "layer2.0.conv2\n",
      "layer2.0.bn2\n",
      "layer2.0.downsample\n",
      "layer2.0.downsample.0\n",
      "layer2.0.downsample.1\n",
      "layer2.1\n",
      "layer2.1.conv1\n",
      "layer2.1.bn1\n",
      "layer2.1.relu\n",
      "layer2.1.conv2\n",
      "layer2.1.bn2\n",
      "layer2.2\n",
      "layer2.2.conv1\n",
      "layer2.2.bn1\n",
      "layer2.2.relu\n",
      "layer2.2.conv2\n",
      "layer2.2.bn2\n",
      "layer2.3\n",
      "layer2.3.conv1\n",
      "layer2.3.bn1\n",
      "layer2.3.relu\n",
      "layer2.3.conv2\n",
      "layer2.3.bn2\n",
      "layer3\n",
      "layer3.0\n",
      "layer3.0.conv1\n",
      "layer3.0.bn1\n",
      "layer3.0.relu\n",
      "layer3.0.conv2\n",
      "layer3.0.bn2\n",
      "layer3.0.downsample\n",
      "layer3.0.downsample.0\n",
      "layer3.0.downsample.1\n",
      "layer3.1\n",
      "layer3.1.conv1\n",
      "layer3.1.bn1\n",
      "layer3.1.relu\n",
      "layer3.1.conv2\n",
      "layer3.1.bn2\n",
      "layer3.2\n",
      "layer3.2.conv1\n",
      "layer3.2.bn1\n",
      "layer3.2.relu\n",
      "layer3.2.conv2\n",
      "layer3.2.bn2\n",
      "layer3.3\n",
      "layer3.3.conv1\n",
      "layer3.3.bn1\n",
      "layer3.3.relu\n",
      "layer3.3.conv2\n",
      "layer3.3.bn2\n",
      "layer3.4\n",
      "layer3.4.conv1\n",
      "layer3.4.bn1\n",
      "layer3.4.relu\n",
      "layer3.4.conv2\n",
      "layer3.4.bn2\n",
      "layer3.5\n",
      "layer3.5.conv1\n",
      "layer3.5.bn1\n",
      "layer3.5.relu\n",
      "layer3.5.conv2\n",
      "layer3.5.bn2\n",
      "layer4\n",
      "layer4.0\n",
      "layer4.0.conv1\n",
      "layer4.0.bn1\n",
      "layer4.0.relu\n",
      "layer4.0.conv2\n",
      "layer4.0.bn2\n",
      "layer4.0.downsample\n",
      "layer4.0.downsample.0\n",
      "layer4.0.downsample.1\n",
      "layer4.1\n",
      "layer4.1.conv1\n",
      "layer4.1.bn1\n",
      "layer4.1.relu\n",
      "layer4.1.conv2\n",
      "layer4.1.bn2\n",
      "layer4.2\n",
      "layer4.2.conv1\n",
      "layer4.2.bn1\n",
      "layer4.2.relu\n",
      "layer4.2.conv2\n",
      "layer4.2.bn2\n",
      "avgpool\n",
      "fc\n"
     ]
    }
   ],
   "source": [
    "for name, _ in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 512]                  --\n",
       "├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n",
       "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-5                        [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-3                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-13                 [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-15                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-16                 [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-18                   [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-6                        [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-19                 [1, 128, 28, 28]          73,728\n",
       "│    │    └─BatchNorm2d: 3-20            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-21                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-22                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-23            [1, 128, 28, 28]          256\n",
       "│    │    └─Sequential: 3-24             [1, 128, 28, 28]          8,448\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-28                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-31                   [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-32                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-33            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-34                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-35                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-36            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-37                   [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-7                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-38                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-39            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-40                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-41                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-42            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-43                   [1, 128, 28, 28]          --\n",
       "├─Sequential: 1-7                        [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-8                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-44                 [1, 256, 14, 14]          294,912\n",
       "│    │    └─BatchNorm2d: 3-45            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-46                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-47                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-48            [1, 256, 14, 14]          512\n",
       "│    │    └─Sequential: 3-49             [1, 256, 14, 14]          33,280\n",
       "│    │    └─ReLU: 3-50                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-9                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-51                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-52            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-53                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-54                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-55            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-56                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-10                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-57                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-58            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-59                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-60                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-61            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-62                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-11                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-63                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-64            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-65                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-66                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-67            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-68                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-12                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-69                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-70            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-71                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-72                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-73            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-74                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-13                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-75                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-76            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-77                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-78                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-79            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-80                   [1, 256, 14, 14]          --\n",
       "├─Sequential: 1-8                        [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-14                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-81                 [1, 512, 7, 7]            1,179,648\n",
       "│    │    └─BatchNorm2d: 3-82            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-83                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-84                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-85            [1, 512, 7, 7]            1,024\n",
       "│    │    └─Sequential: 3-86             [1, 512, 7, 7]            132,096\n",
       "│    │    └─ReLU: 3-87                   [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-15                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-88                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-89            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-90                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-91                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-92            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-93                   [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-16                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-94                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-95            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-96                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-97                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-98            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-99                   [1, 512, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "├─Identity: 1-10                         [1, 512]                  --\n",
       "==========================================================================================\n",
       "Total params: 21,284,672\n",
       "Trainable params: 21,284,672\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 3.66\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 59.81\n",
       "Params size (MB): 85.14\n",
       "Estimated Total Size (MB): 145.55\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, (1, 3, 224, 224), depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving './models/resnet34_pretrained.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gio: http://localhost:8080: Operation not supported\n"
     ]
    }
   ],
   "source": [
    "import netron\n",
    "\n",
    "torch_input = torch.randn(1, 3, 512, 512)\n",
    "model = model.cuda()  # Move the model to CUDA\n",
    "torch_input = torch_input.cuda()  # Move the input tensor to CUDA\n",
    "# onnx_program = torch.onnx.dynamo_export(model, torch_input)\n",
    "onnx_path = \"./models/resnet34_pretrained.onnx\"\n",
    "# onnx_program.save(onnx_path)\n",
    "torch.onnx.export(model, torch_input, onnx_path)\n",
    "netron.start(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "netron.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizePerImage(object):\n",
    "    def __call__(self, tensor):\n",
    "        # Compute the mean and std for each individual image\n",
    "        mean = tensor.mean([1, 2], keepdim=True)  # Compute mean along height and width\n",
    "        std = tensor.std([1, 2], keepdim=True)    # Compute std along height and width\n",
    "        # Normalize the image by subtracting the mean and dividing by the std\n",
    "        return (tensor - mean) / (std + 1e-7)     # Adding epsilon to avoid division by zero\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    NormalizePerImage(),  # Apply the per-image normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the directory path\n",
    "img_path = \"./img/\"\n",
    "output_base_path = \"./img_output/\"  # Base output folder for processed images\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(output_base_path):\n",
    "    os.makedirs(output_base_path)\n",
    "\n",
    "# Rename images sequentially in the format `0.png`, `1.png`, etc.\n",
    "\n",
    "\n",
    "def rename_images_in_folder(img_folder):\n",
    "    images = [f for f in os.listdir(img_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    images.sort()  # Sort images to ensure consistent renaming order\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        ext = os.path.splitext(image)[1]  # Get the file extension (e.g., .png, .jpg)\n",
    "        new_name = f\"{idx}{ext}\"\n",
    "        old_image_path = os.path.join(img_folder, image)\n",
    "        new_image_path = os.path.join(img_folder, new_name)\n",
    "\n",
    "        # Rename the image\n",
    "        os.rename(old_image_path, new_image_path)\n",
    "\n",
    "\n",
    "# Rename the images\n",
    "rename_images_in_folder(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, fname in enumerate(os.listdir(img_path)):\n",
    "    if fname.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        img_full_path = os.path.join(img_path, fname)\n",
    "        img = Image.open(img_full_path).convert('L')  # Convert to grayscale ('L')\n",
    "        folder_name = os.path.join(output_base_path, str(idx))\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "        # Save the grayscale image to the output directory\n",
    "        img.save(os.path.join(folder_name, f\"{idx}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom DatasetFolder to handle images without class folders\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Gather all image file paths in the directory\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        # Open the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path  # No label, just return the image and its path\n",
    "\n",
    "\n",
    "# Load the renamed images into a DataLoader\n",
    "dataset = TestImageDataset(root_dir=img_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_layer_names = [\n",
    "    'layer1.0.relu', 'layer1.1.relu', 'layer1.2.relu',\n",
    "    # 'layer2.0.relu', 'layer2.1.relu', 'layer2.2.relu', 'layer2.3.relu',\n",
    "    # 'layer3.0.relu', 'layer3.1.relu', 'layer3.2.relu', 'layer3.3.relu', 'layer3.4.relu', 'layer3.5.relu',\n",
    "    # 'layer4.0.relu', 'layer4.1.relu', 'layer4.2.relu'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_maps(model, selected_layer_names, input_image, relu_invocations):\n",
    "    feature_maps = {}\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # Find the layer's name in the model by matching the module object\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                # feature_maps[name] = output.detach()\n",
    "                # Increase the count of ReLU invocations\n",
    "                relu_invocations[name] += 1\n",
    "                # print(f\"Layer: {name}, ReLU invocations: {relu_invocations[name]}\")\n",
    "\n",
    "                # Store the output only for the first ReLU invocation (residual branch)\n",
    "                if relu_invocations[name] == 1:\n",
    "                    feature_maps[name] = output.detach()\n",
    "                break\n",
    "\n",
    "    hooks = []\n",
    "    # Register hooks using the layer names\n",
    "    for name, module in model.named_modules():\n",
    "        # print(name)\n",
    "        if name in selected_layer_names:\n",
    "            # print('find')\n",
    "            hook = module.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    # Forward pass to extract feature maps\n",
    "    model(input_image)\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def apply_colormap_and_save(fmap, output_path, colormap='jet'):\n",
    "    # Normalize the feature map to [0, 1] for visualization purposes\n",
    "    fmap = fmap - fmap.min()  # Shift to make minimum 0\n",
    "    fmap = fmap / fmap.max()  # Scale to make maximum 1\n",
    "\n",
    "    # Convert the feature map to a numpy array\n",
    "    fmap_np = fmap.cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    # Plot the attention map using imshow with the specified colormap\n",
    "    heatmap = ax.imshow(fmap_np, cmap=colormap)\n",
    "\n",
    "    # Turn off x-axis and y-axis\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Add a color bar to the right of the plot\n",
    "    cbar = plt.colorbar(heatmap, ax=ax)\n",
    "\n",
    "    # Save the plot (including the color bar)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr, laplace\n",
    "\n",
    "\n",
    "def compute_bins(fmap_np, rule='freedman-diaconis'):\n",
    "    n = len(fmap_np)\n",
    "    if n <= 1:\n",
    "        return 10  # Default minimum number of bins for very small datasets\n",
    "\n",
    "    data_range = fmap_np.max() - fmap_np.min()  # Range of the data\n",
    "\n",
    "    if rule == 'sturges':\n",
    "        # Sturges' rule\n",
    "        return int(np.ceil(np.log2(n) + 1))\n",
    "\n",
    "    elif rule == 'scott':\n",
    "        # Scott's rule\n",
    "        std = np.std(fmap_np)\n",
    "        if std == 0:  # Handle the case where the standard deviation is zero\n",
    "            return 10  # Default number of bins\n",
    "        bin_width = 3.5 * std / (n ** (1 / 3))\n",
    "        return int(np.ceil(data_range / bin_width))\n",
    "\n",
    "    elif rule == 'freedman-diaconis':\n",
    "        # Freedman-Diaconis rule\n",
    "        iqr_value = iqr(fmap_np)\n",
    "        if iqr_value == 0:  # Handle the case where the IQR is zero\n",
    "            return 10  # Default number of bins\n",
    "        bin_width = 2 * iqr_value / (n ** (1 / 3))\n",
    "        if bin_width == 0:  # Ensure no division by zero\n",
    "            return 10  # Default number of bins\n",
    "        return int(np.ceil(data_range / bin_width))\n",
    "\n",
    "    else:\n",
    "        # Default to Sturges' rule if an unknown rule is provided\n",
    "        return int(np.ceil(np.log2(n) + 1))\n",
    "\n",
    "# Function to plot histogram with Laplace PDF and adaptive bins\n",
    "\n",
    "\n",
    "def percentage_formatter(x, pos):\n",
    "    return f'{x:.1f}%'\n",
    "\n",
    "\n",
    "def plot_histogram_with_laplace(fmap, output_path, binning_rule='freedman-diaconis', bin_spacing=0.95):\n",
    "    # Flatten the feature map to get all values in a single array\n",
    "    fmap_np = fmap.cpu().numpy().flatten()\n",
    "\n",
    "    # Compute the number of bins adaptively\n",
    "    num_bins = compute_bins(fmap_np, rule=binning_rule)\n",
    "\n",
    "    # Compute the mean and standard deviation of the feature map\n",
    "    mean = np.mean(fmap_np)\n",
    "    std = np.std(fmap_np)\n",
    "\n",
    "    # Create a histogram\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot the histogram with the computed number of bins\n",
    "    counts, bins, patches = ax1.hist(fmap_np, bins=num_bins, density=False, alpha=0.6, color='g', label=\"Channel Features\", edgecolor='black', rwidth=bin_spacing)\n",
    "\n",
    "    # Manually normalize the counts to represent percentages\n",
    "    total_counts = np.sum(counts)  # Total number of counts\n",
    "    percentages = (counts / total_counts) * 100  # Convert to percentage\n",
    "\n",
    "    # Clear the plot and plot the percentages instead of counts\n",
    "    ax1.cla()  # Clear the current axis\n",
    "    ax1.bar(bins[:-1], percentages, width=np.diff(bins) * bin_spacing, edgecolor='black', align='edge', color='g', alpha=0.6, label=\"Channel Features\")\n",
    "\n",
    "    # Set labels and title for the histogram\n",
    "    ax1.set_xlabel('Feature Value')\n",
    "    ax1.set_ylabel('Percentage')\n",
    "    # ax1.set_title(f'Histogram and Laplace PDF (mean={mean:.2f}, std={std:.2f})')\n",
    "\n",
    "    ax1.yaxis.set_major_formatter(FuncFormatter(percentage_formatter))\n",
    "\n",
    "    if std == 0:\n",
    "        # If standard deviation is zero, set Laplace PDF to a constant (e.g., 1) to avoid division by zero\n",
    "        laplace_pdf = np.ones_like(bins)  # Set PDF to a constant value (or skip this part entirely)\n",
    "    else:\n",
    "        # Compute the Laplace distribution using scipy's laplace function\n",
    "        laplace_pdf = laplace.pdf(bins, loc=mean, scale=std / np.sqrt(2))  # Laplace scale = std / sqrt(2)\n",
    "\n",
    "    # Create a secondary y-axis to plot the Laplace PDF\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(bins, laplace_pdf, 'r-', label=\"Fitted Laplace Distribution Amplitude\")\n",
    "    ax2.set_ylabel('Fitted Laplace Distribution Amplitude', color='r')\n",
    "\n",
    "    # Set limits and ticks for both axes\n",
    "    # ax1.set_ylim(0, max(percentages) * 1.2)  # Adjust histogram y-axis limit for better visibility\n",
    "\n",
    "    # # Handle the case where the Laplace PDF may contain invalid values\n",
    "    # if np.all(np.isfinite(laplace_pdf)):  # Check if all values are finite (not NaN or Inf)\n",
    "    #     ax2.set_ylim(0, max(laplace_pdf) * 1.2)  # Adjust Laplace y-axis limit\n",
    "    # else:\n",
    "    #     ax2.set_ylim(0, 1)  # Set a default y-limit if PDF contains NaN or Inf\n",
    "    ax1.set_ylim(0, 8.0)\n",
    "    ax2.set_ylim(0, 8)\n",
    "\n",
    "    ax1.set_xlim(0, 1)\n",
    "\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # # Add legends\n",
    "    # ax1.legend(loc=\"upper left\")\n",
    "    # ax2.legend(loc=\"upper right\")\n",
    "    # Combine legends from both axes\n",
    "    handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(handles1 + handles2, labels1 + labels2, loc=\"upper right\")  # Set combined legend position\n",
    "\n",
    "    # Save the plot as an image\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_map_and_save(fmap, output_path, colormap='jet'):\n",
    "    # Flatten the feature map to get all values in a single array for computing statistics\n",
    "    fmap_np = fmap.cpu().numpy().flatten()\n",
    "\n",
    "    # Compute the mean and standard deviation of the feature map\n",
    "    mean = np.mean(fmap_np)\n",
    "    std = np.std(fmap_np)\n",
    "\n",
    "    # Compute the Laplace PDF using scipy's laplace function\n",
    "    laplace_pdf = laplace.pdf(fmap_np, loc=mean, scale=std / np.sqrt(2))  # Laplace scale = std / sqrt(2)\n",
    "\n",
    "    # Convert Laplace PDF to a PyTorch tensor\n",
    "    laplace_pdf_tensor = torch.tensor(laplace_pdf).to(fmap.device)\n",
    "\n",
    "    # Compute the attention weights using torch.sigmoid(1 / (lap_pdf + 1))\n",
    "    attention_weights = torch.sigmoid(1 / (laplace_pdf_tensor + 1))\n",
    "\n",
    "    # Reshape the attention weights back to the original feature map shape\n",
    "    attention_weights = attention_weights.reshape(fmap.shape).cpu().numpy()\n",
    "\n",
    "    # # Normalize the attention weights to [0, 1] for visualization\n",
    "    # attention_weights = attention_weights - attention_weights.min()\n",
    "    # attention_weights = attention_weights / attention_weights.max()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    # Plot the attention map using imshow with the specified colormap\n",
    "    heatmap = ax.imshow(attention_weights, cmap=colormap, vmin=0, vmax=1)\n",
    "\n",
    "    # Add a color bar to the right of the plot\n",
    "    cbar = plt.colorbar(heatmap, ax=ax)\n",
    "\n",
    "    # Save the plot (including the color bar)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifeng/miniconda3/envs/attention/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:2027: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2/6...\n",
      "Processing batch 3/6...\n",
      "Processing batch 4/6...\n",
      "Processing batch 5/6...\n",
      "Processing batch 6/6...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for i, (input_images, _) in enumerate(dataloader):\n",
    "    input_images = input_images.to(device)\n",
    "    print(f\"Processing batch {i+1}/{len(dataloader)}...\")\n",
    "\n",
    "    for j, input_image in enumerate(input_images):\n",
    "        input_image = input_image.unsqueeze(0)  # Add batch dimension if necessary\n",
    "        input_image = input_image.to(device)  # Move to the same device as the model\n",
    "\n",
    "        # Initialize relu_invocations for the current image\n",
    "        relu_invocations = {name: 0 for name in selected_layer_names}\n",
    "\n",
    "        # Extract feature maps for the current image\n",
    "        feature_maps = extract_feature_maps(model, selected_layer_names, input_image, relu_invocations)\n",
    "\n",
    "        for layer_name, fmap in feature_maps.items():\n",
    "            # Squeeze batch dimension if necessary (assuming the batch size is always 1)\n",
    "            if fmap.dim() > 3:\n",
    "                fmap = fmap.squeeze(0)\n",
    "\n",
    "            C, H, W = fmap.shape\n",
    "            for c in range(C):\n",
    "                # Create a folder for each feature map\n",
    "                fmap_folder_name = os.path.join(output_base_path, str(i), layer_name, \"channel_feature_maps\")\n",
    "                if not os.path.exists(fmap_folder_name):\n",
    "                    os.makedirs(fmap_folder_name)\n",
    "\n",
    "                output_path_colormap = os.path.join(fmap_folder_name, f\"{c}.png\")\n",
    "                apply_colormap_and_save(fmap[c], output_path_colormap)\n",
    "\n",
    "                # Save the histogram with Laplace PDF\n",
    "                histogram_folder_name = os.path.join(output_base_path, str(i), layer_name, \"channel_histogram\")\n",
    "                if not os.path.exists(histogram_folder_name):\n",
    "                    os.makedirs(histogram_folder_name)\n",
    "\n",
    "                output_path_histogram = os.path.join(histogram_folder_name, f\"{c}_histogram.png\")\n",
    "                plot_histogram_with_laplace(fmap[c], output_path_histogram)\n",
    "\n",
    "                attention_folder_name = os.path.join(output_base_path, str(i), layer_name, \"channel_attention\")\n",
    "                if not os.path.exists(attention_folder_name):\n",
    "                    os.makedirs(attention_folder_name)\n",
    "\n",
    "                output_path_attention = os.path.join(attention_folder_name, f\"{c}_attention.png\")\n",
    "                compute_attention_map_and_save(fmap[c], output_path_attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
